/**
 * OpenAI Provider Service
 * Gestion des interactions avec l'API OpenAI GPT-4
 * Compatible avec serveur AI personnel (ai.aenews.net)
 */

// Environment variables loaded by server.js
import axios from 'axios';

const OPENAI_API_KEY = process.env.OPENAI_API_KEY || '';
const OPENAI_BASE_URL = process.env.OPENAI_BASE_URL || 'https://ai.aenews.net';
const OPENAI_MODEL = process.env.OPENAI_MODEL || 'gpt-4o-mini';
const OPENAI_MAX_TOKENS = parseInt(process.env.OPENAI_MAX_TOKENS) || 4000;
const OPENAI_TEMPERATURE = parseFloat(process.env.OPENAI_TEMPERATURE) || 0.7;

// Construction de l'URL de l'API (compatible IA-CORE AENEWS)
const OPENAI_API_URL = `${OPENAI_BASE_URL}/api/chat`;

console.log('[OpenAI Provider] Configuration:');
console.log(`  - Base URL: ${OPENAI_BASE_URL}`);
console.log(`  - API URL: ${OPENAI_API_URL}`);
console.log(`  - Model: ${OPENAI_MODEL}`);
console.log(`  - API Key: ${OPENAI_API_KEY ? OPENAI_API_KEY.substring(0, 8) + '...' : 'NOT SET'}`);

/**
 * Syst√®me de prompts pour l'agent DevOps
 */
const SYSTEM_PROMPTS = {
    devops_agent: `Tu es un Agent DevOps IA expert. Tu aides les utilisateurs √† :
- Analyser et auditer du code (Node.js, Python, PHP)
- Configurer des serveurs et services (Nginx, Apache, PM2, Docker)
- D√©bugger des applications
- Optimiser les performances
- S√©curiser les syst√®mes
- G√©rer des bases de donn√©es

R√®gles importantes :
1. TOUJOURS demander confirmation avant des actions critiques (suppression, modifications syst√®me)
2. Pour les actions s√ªres (lecture, analyse, npm install), agir de mani√®re autonome
3. Cr√©er des backups automatiques avant toute modification de fichier
4. Expliquer clairement ce que tu fais et pourquoi
5. Proposer des actions avec des boutons cliquables : [Action] ou [Alternative]
6. Formater le code avec \`\`\`language pour une meilleure lisibilit√©
7. Utiliser des emojis pour rendre les messages plus clairs (‚úÖ‚ùå‚ö†Ô∏èüí°üîß)

Tu as acc√®s aux capacit√©s suivantes via des commandes :
- ANALYZE_CODE(path) - Analyser du code
- READ_FILE(path) - Lire un fichier
- WRITE_FILE(path, content) - √âcrire dans un fichier
- EXECUTE_COMMAND(command) - Ex√©cuter une commande SSH
- AUDIT_SECURITY() - Audit de s√©curit√©
- AUDIT_DOCKER() - Audit Docker
- GET_METRICS() - Obtenir les m√©triques syst√®me

Format de r√©ponse :
1. R√©sum√© de la demande
2. Actions propos√©es avec niveau de risque
3. Boutons d'action pour l'utilisateur
4. Explication d√©taill√©e si n√©cessaire`,

    code_analyzer: `Tu es un expert en analyse de code. Analyse le code fourni et identifie :
- Bugs potentiels
- Probl√®mes de s√©curit√©
- Anti-patterns
- Optimisations possibles
- Conformit√© aux bonnes pratiques

Fournis un rapport structur√© avec :
1. R√©sum√© (nombre de probl√®mes par criticit√©)
2. D√©tails de chaque probl√®me avec ligne de code
3. Suggestions de correction avec code exemple`,

    security_auditor: `Tu es un expert en s√©curit√©. Audite l'application et identifie :
- Vuln√©rabilit√©s connues (CVE)
- Mauvaises pratiques de s√©curit√©
- Credentials expos√©s
- Configurations dangereuses
- Permissions incorrectes

Fournis un rapport avec :
1. Score de s√©curit√© (0-100)
2. Liste des vuln√©rabilit√©s par criticit√©
3. Actions correctives recommand√©es`,

    docker_expert: `Tu es un expert Docker. Analyse les configurations Docker et identifie :
- Dockerfiles non optimis√©s
- Images vuln√©rables
- Mauvaises pratiques
- Probl√®mes de performance
- Optimisations possibles

Fournis des recommandations concr√®tes avec exemples de code.`
};

/**
 * Classification des actions par niveau de risque
 */
export const ACTION_RISK_LEVELS = {
    // Niveau 0 : Lecture seule (autonome)
    SAFE: {
        level: 0,
        color: 'green',
        requiresConfirmation: false,
        actions: [
            'READ_FILE',
            'ANALYZE_CODE',
            'GET_METRICS',
            'LIST_FILES',
            'SHOW_LOGS',
            'GIT_STATUS',
            'NPM_LIST',
            'DOCKER_PS',
            'PS_AUX'
        ]
    },
    
    // Niveau 1 : Actions mod√©r√©es (confirmation simple)
    MODERATE: {
        level: 1,
        color: 'yellow',
        requiresConfirmation: true,
        confirmationType: 'simple',
        actions: [
            'WRITE_FILE',
            'NPM_INSTALL',
            'NPM_UPDATE',
            'PIP_INSTALL',
            'GIT_PULL',
            'PM2_RESTART',
            'NGINX_RELOAD',
            'CREATE_BACKUP',
            'MODIFY_CONFIG'
        ]
    },
    
    // Niveau 2 : Actions critiques (confirmation d√©taill√©e)
    CRITICAL: {
        level: 2,
        color: 'red',
        requiresConfirmation: true,
        confirmationType: 'detailed',
        actions: [
            'DELETE_FILE',
            'RM_RF',
            'DROP_DATABASE',
            'GIT_PUSH_FORCE',
            'DOCKER_RM',
            'DOCKER_STOP',
            'CHMOD_SYSTEM',
            'UFW_DISABLE',
            'SYSTEMCTL_STOP'
        ]
    }
};

/**
 * Envoie une requ√™te √† l'API (OpenAI ou serveur personnel)
 * @param {Array} messages - Historique de la conversation
 * @param {string} systemPrompt - Prompt syst√®me √† utiliser
 * @returns {Promise<Object>} R√©ponse de l'IA
 */
export async function sendToOpenAI(messages, systemPrompt = 'devops_agent') {
    if (!OPENAI_API_KEY) {
        throw new Error('OpenAI API key not configured');
    }

    try {
        console.log(`[OpenAI Provider] Sending request to ${OPENAI_API_URL}`);
        console.log(`[OpenAI Provider] Model: ${OPENAI_MODEL}`);
        console.log(`[OpenAI Provider] Messages count: ${messages.length + 1}`);

        const response = await axios.post(
            OPENAI_API_URL,
            {
                model: OPENAI_MODEL,
                messages: [
                    { role: 'system', content: SYSTEM_PROMPTS[systemPrompt] },
                    ...messages
                ],
                max_tokens: OPENAI_MAX_TOKENS,
                temperature: OPENAI_TEMPERATURE,
                presence_penalty: 0.6,
                frequency_penalty: 0.3
            },
            {
                headers: {
                    'X-API-Key': OPENAI_API_KEY,  // IA-CORE AENEWS (M√©thode principale)
                    'Authorization': `Bearer ${OPENAI_API_KEY}`,  // Fallback
                    'Content-Type': 'application/json'
                },
                timeout: 90000 // 90s timeout (pour cold start IA-CORE)
            }
        );

        console.log('[OpenAI Provider] Response received successfully');
        console.log(`[OpenAI Provider] Model used: ${response.data.model || OPENAI_MODEL}`);

        return {
            success: true,
            message: response.data.choices[0].message.content,
            usage: response.data.usage,
            model: response.data.model || OPENAI_MODEL
        };
    } catch (error) {
        console.error('[OpenAI Provider] API Error:', error.response?.data || error.message);
        
        if (error.response?.status === 401) {
            throw new Error('Invalid API key');
        } else if (error.response?.status === 429) {
            throw new Error('Rate limit exceeded. Please try again later.');
        } else if (error.code === 'ECONNABORTED') {
            throw new Error('Request timeout. The model took too long to respond.');
        } else if (error.response?.status === 404) {
            throw new Error(`API endpoint not found: ${OPENAI_API_URL}`);
        } else {
            throw new Error(`API error: ${error.message}`);
        }
    }
}

/**
 * Analyse du code avec GPT-4
 * @param {string} code - Code √† analyser
 * @param {string} language - Langage de programmation
 * @param {string} filename - Nom du fichier
 * @returns {Promise<Object>} Analyse du code
 */
export async function analyzeCodeWithAI(code, language, filename) {
    const messages = [
        {
            role: 'user',
            content: `Analyse ce code ${language} du fichier "${filename}" :\n\n\`\`\`${language}\n${code}\n\`\`\`\n\nIdentifie les bugs, probl√®mes de s√©curit√©, et optimisations possibles.`
        }
    ];

    return await sendToOpenAI(messages, 'code_analyzer');
}

/**
 * Audit de s√©curit√© avec GPT-4
 * @param {Object} context - Contexte de l'application (fichiers, config, etc.)
 * @returns {Promise<Object>} Rapport d'audit
 */
export async function securityAuditWithAI(context) {
    const messages = [
        {
            role: 'user',
            content: `Effectue un audit de s√©curit√© de cette application :\n\nPackage.json:\n\`\`\`json\n${JSON.stringify(context.packageJson, null, 2)}\n\`\`\`\n\nFichiers de configuration:\n${context.configFiles.map(f => `- ${f.path}`).join('\n')}\n\nIdentifie toutes les vuln√©rabilit√©s et propose des corrections.`
        }
    ];

    return await sendToOpenAI(messages, 'security_auditor');
}

/**
 * Expertise Docker avec GPT-4
 * @param {string} dockerfile - Contenu du Dockerfile
 * @param {Array} containers - Liste des containers
 * @returns {Promise<Object>} Analyse Docker
 */
export async function dockerExpertiseWithAI(dockerfile, containers) {
    const messages = [
        {
            role: 'user',
            content: `Analyse cette configuration Docker :\n\nDockerfile:\n\`\`\`dockerfile\n${dockerfile}\n\`\`\`\n\nContainers actifs:\n${containers.map(c => `- ${c.name} (${c.image})`).join('\n')}\n\nPropose des optimisations et identifie les probl√®mes.`
        }
    ];

    return await sendToOpenAI(messages, 'docker_expert');
}

/**
 * Conversation g√©n√©rale avec l'agent DevOps
 * @param {Array} conversationHistory - Historique de la conversation
 * @param {string} userMessage - Nouveau message de l'utilisateur
 * @param {Object} context - Contexte actuel (serveur, fichiers ouverts, etc.)
 * @returns {Promise<Object>} R√©ponse de l'agent
 */
export async function chatWithAgent(conversationHistory, userMessage, context = {}) {
    // Pr√©parer le contexte pour l'IA
    let contextString = '';
    if (context.server) {
        contextString += `\n[Serveur actif: ${context.server.name} (${context.server.host})]`;
    }
    if (context.currentDirectory) {
        contextString += `\n[R√©pertoire: ${context.currentDirectory}]`;
    }
    if (context.openFiles && context.openFiles.length > 0) {
        contextString += `\n[Fichiers ouverts: ${context.openFiles.join(', ')}]`;
    }

    const messages = [
        ...conversationHistory,
        {
            role: 'user',
            content: contextString ? `${contextString}\n\n${userMessage}` : userMessage
        }
    ];

    const response = await sendToOpenAI(messages, 'devops_agent');
    
    // Parser la r√©ponse pour extraire les actions propos√©es
    const actions = extractActionsFromResponse(response.message);
    
    return {
        ...response,
        actions,
        requiresConfirmation: actions.some(a => a.requiresConfirmation)
    };
}

/**
 * Extrait les actions de la r√©ponse de l'IA
 * @param {string} message - Message de l'IA
 * @returns {Array} Liste des actions d√©tect√©es
 */
function extractActionsFromResponse(message) {
    const actions = [];
    const actionPatterns = [
        { pattern: /ANALYZE_CODE\((.*?)\)/g, type: 'ANALYZE_CODE', risk: 'SAFE' },
        { pattern: /READ_FILE\((.*?)\)/g, type: 'READ_FILE', risk: 'SAFE' },
        { pattern: /WRITE_FILE\((.*?)\)/g, type: 'WRITE_FILE', risk: 'MODERATE' },
        { pattern: /EXECUTE_COMMAND\((.*?)\)/g, type: 'EXECUTE_COMMAND', risk: 'MODERATE' },
        { pattern: /DELETE_FILE\((.*?)\)/g, type: 'DELETE_FILE', risk: 'CRITICAL' }
    ];

    actionPatterns.forEach(({ pattern, type, risk }) => {
        let match;
        while ((match = pattern.exec(message)) !== null) {
            actions.push({
                type,
                params: match[1],
                risk,
                requiresConfirmation: ACTION_RISK_LEVELS[risk].requiresConfirmation
            });
        }
    });

    return actions;
}

/**
 * G√©n√®re une suggestion de correction de code
 * @param {string} code - Code avec bug
 * @param {string} bugDescription - Description du bug
 * @returns {Promise<string>} Code corrig√©
 */
export async function suggestCodeFix(code, bugDescription) {
    const messages = [
        {
            role: 'user',
            content: `Corrige ce bug dans le code suivant :\n\nBug: ${bugDescription}\n\nCode:\n\`\`\`\n${code}\n\`\`\`\n\nDonne uniquement le code corrig√©, sans explication.`
        }
    ];

    const response = await sendToOpenAI(messages, 'code_analyzer');
    
    // Extraire le code corrig√© des balises ```
    const codeMatch = response.message.match(/```(?:\w+)?\n([\s\S]*?)\n```/);
    return codeMatch ? codeMatch[1] : response.message;
}

/**
 * V√©rifie si l'API OpenAI est configur√©e
 * @returns {boolean}
 */
export function isOpenAIConfigured() {
    return Boolean(OPENAI_API_KEY && OPENAI_API_KEY !== 'your-openai-api-key-here');
}

/**
 * Teste la connexion √† l'API OpenAI
 * @returns {Promise<Object>}
 */
export async function testOpenAIConnection() {
    try {
        const response = await sendToOpenAI([
            { role: 'user', content: 'Test de connexion. R√©ponds simplement "OK".' }
        ]);
        
        return {
            success: true,
            message: 'AI API connected successfully',
            model: response.model,
            baseUrl: OPENAI_BASE_URL
        };
    } catch (error) {
        return {
            success: false,
            error: error.message,
            baseUrl: OPENAI_BASE_URL
        };
    }
}
